---
title: "regression"
author: "Kameni"
date: "Wednesday, November 12, 2014"
output: html_document
---
```{r}
swirl()

de


```

we'll show how to eliminate any chosen regressor, thus reducing a regression in N variables, to a regression in N-1.

 if we know how to do a regression in 1 variable, we can do a regression in 2. Once we know how to do a regression in 2 variables, we can do a regression in 3, and so on.


when we perform a regresssion in one variable, such aslm(child~parent,galton), we get two coefficients, a slope and an intercept. The intercept is really th coefficient of a special regressor which has the same value,1, at every sample. The function lm includes this regressor by default.

We will demonstrate by substituting n all-ones regressor of our own. This regressor must have the same number of samples as galton(928). We want to create such an object which we will name "ones"

```{r}
ones<-rep(1,nrow(galton))

```

we will exclude to default intercept by using -1 in the formula

```{r}
lm(child~ones+parent-1,galton)
#Coefficients:
#   ones   parent  
#23.9415   0.6463
```

The coefficient of ones is 23.9415.If we now use the default, we will see that the intercept has the same value

```{rfr}
lm(child~ones+parent,galton)
#Coefficients:
#(Intercept)         ones       parent  
#    23.9415           NA       0.6463  

```


 we could see in arlier lessons that the regression line given by lm(child~parent,galton) goes through the point x=mean(child),y=mean(parent). we know also that, the regression line goes through origin,(x=0,y=0),if we subtract the mean from each variable. that means, we  eliminate one of the two regressors,the constant, by  subtracting the means. The remaining regressor is the slope.
 


#Gaussian Elimination
is a elimination technique which consists of picking one regressor and replace all other variables by residuals of their regressions against one.

The mean of a variable is the coefficient of its regression against the constant, 1. 

```{r}
lm(child ~ 1, galton)
```

#illustrate the general case 
The idea is to predict the Volume of timber which a tree might produce from measurements of its Height and Girth.

To avoid treating the intercept as a special case, we have added a column of 1's to the data which we shall use in its place.
```{r}
require(dataset)
head(trees)
```

The general technique is to pick one predictor and to replace all other variables by the residuals of their regressions against that one. The function, regressOneOnOne, in eliminate.R performs the first step of this process. Given the name of a predictor and one other variable, other, it returns the residual of other when regressed against predictor. In its first line, labeled Point A, it creates a formula. Suppose that predictor were 'Girth' and other were 'Volume'. What formula would it create?

1: Volume ~ Girth - 1 <<<<<< answers
2: Volume ~ Girth
3: Girth ~ Volume - 1




The remaining function, eliminate, applies regressOneOnOne to all variables except a given predictor and collects the residuals in a data frame. We'll first show that when we eliminate one regressor from the data, a regression on the remaining will produce their correct coefficients. (Of course, the coefficient of the eliminated regressor will be missing, but more about that later.)


For reference, create a model named fit, based on all three regressors, Girth, Height, and Constant, and assign the result to a variable named fit. Use an expression such as fit <- lm(Volume ~ Girth + Height + Constant -1, trees). Don't forget the -1, and be sure to name the model fit for later use.

```{r}
fit<-lm(Volume~Girth + Height + Constant -1,trees)
```

Now let's eliminate Girth from the data set. Call the reduced data set trees2 to indicate it has only 2 regressors.

```{r}
trees2 <- eliminate("Girth", trees)
#Use head(trees2) or View(trees2) to inspect the reduced data set.
head(trees2)
```
we can see it:The constant, 1, has been replaced by its residual when regressed against Girth.


Now create a model, called fit2, using the reduced data set. Use an expression such as fit2 <- lm(Volume ~ Height + Constant -1, trees2). Don't forget to use -1 in the formula.

```{r}
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
#Use the expression lapply(list(fit, fit2), coef) to print coefficients of fit and fit2 for comparison.

lapply(list(fit, fit2), coef)
```


 The coefficient of the eliminated variable is missing, of course. One way to get it would be to go back to the original data, trees, eliminate a different regressor, such as Height, and do another 2 variable regession, as above. There are much more efficient ways, but efficiency is not the point of this demonstration. We have shown how to reduce a regression in 3 variables to a regression in 2. We can go further and eliminate another variable, reducing a regression in 2 variables to a regression in 1.


```{r}
lm(formula = Volume ~ Constant - 1, data = eliminate("Height",trees2))

```


Suppose we were given a multivariable regression problem involving an outcome and N regressors, where N > 1. Using only single-variable regression, how can the problem be reduced to a problem with only N-1 regressors?

```{r}
1: Pick any regressor and replace the outcome and all other regressors by their residuals against the chosen one.
```


#conclusion
 We have illustrated that regression in many variables amounts to a series of regressions in one. The actual algorithms used by functions such as lm are more efficient, but are computationally equivalent to what we have done. That is, the algorithms use equivalent steps but combine them more efficiently and abstractly. This completes the lesson.


# Function


we create a function to regress the given variable on the given predictor. the function  suppresses the intercept, and returns the residual.
```{r}
regressOneOnOne <- function(predictor, other, dataframe){
  # Point A. Create a formula such as Girth ~ Height -1
  formula <- paste0(other, " ~ ", predictor, " - 1")
  # Use the formula in a regression and return the residual.
  resid(lm(formula, dataframe))
}
```

now we eliminate the specified predictor from the dataframe by
 regressing all other variables on that predictor and returning a data frame containing the residuals of those regressions.

```{r}
eliminate <- function(predictor, dataframe){
  # Find the names of all columns except the predictor.
  others <- setdiff(names(dataframe), predictor)
  # Calculate the residuals of each when regressed against the given predictor
  temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
  # sapply returns a matrix of residuals; convert to a data frame and return.
  as.data.frame(temp)
}
```

